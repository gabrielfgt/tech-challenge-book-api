# Pipeline de Processamento de Dados de Livros

Esta pipeline executa ETL (Extract, Transform, Load) completo nos dados de livros, realizando limpeza bÃ¡sica e feature engineering avanÃ§ado.

## Estrutura da Pipeline

### ğŸ“ Arquivos Principais

- `data_types.py` - Modelos Pydantic e schemas de validaÃ§Ã£o
- `cleaning_pipeline.py` - Pipeline de limpeza bÃ¡sica dos dados
- `feature_pipeline.py` - Pipeline de feature engineering
- `main_pipeline.py` - OrquestraÃ§Ã£o completa da pipeline
- `run_pipeline.py` - Script executÃ¡vel com CLI

## ğŸ§¹ Pipeline de Limpeza (Fase 1)

**Entrada:** `data/raw/all_books_with_images.csv`  
**SaÃ­da:** `data/processed/books_processed.csv`

### OperaÃ§Ãµes Realizadas:

1. **VerificaÃ§Ã£o de nulos** - Valida integridade dos dados
2. **CriaÃ§Ã£o de ID Ãºnico** - Adiciona coluna `id` com identificadores Ãºnicos
3. **Limpeza de categorias** - Substitui categorias problemÃ¡ticas ('Add a comment', 'Default') por 'Outros'
4. **TransformaÃ§Ã£o availability** - Converte 'yes'/'no' para 1/0
5. **ValidaÃ§Ã£o final** - Garante consistÃªncia dos dados processados

### Resultados da Limpeza:
- âœ… 1000 registros processados sem nulos
- ğŸ·ï¸ 219 categorias problemÃ¡ticas limpas
- ğŸ”¢ 1000 IDs Ãºnicos criados

## âš™ï¸ Pipeline de Feature Engineering (Fase 2)

**Entrada:** `data/processed/books_processed.csv`  
**SaÃ­da:** `data/features/books_features.csv`

### Features Criadas (59 total):

#### 1. **CategorizaÃ§Ã£o de PreÃ§os**
- `price_range`: Baixo (â‰¤20), MÃ©dio (20-40), Alto (40-50), Premium (>50)

#### 2. **Features de TÃ­tulo** (6 features)
- `has_subtitle`: TÃ­tulo contÃ©m ':' (307 livros)
- `has_series`: TÃ­tulo contÃ©m '(' (345 livros) 
- `starts_with_the`: ComeÃ§a com 'The' (269 livros)
- `title_length`: Comprimento em caracteres (mÃ©dia: 39.1)
- `title_word_count`: NÃºmero de palavras (mÃ©dia: 6.7)
- `has_numbers`: ContÃ©m nÃºmeros (365 livros)

#### 3. **CategorizaÃ§Ã£o de Ratings**
- `rating_category`: Muito Baixo/Baixo/MÃ©dio/Alto/Muito Alto

#### 4. **NÃ­veis de Stock**
- `stock_level`: Baixo (1-5), MÃ©dio (6-15), Alto (16+)

#### 5. **Score de Popularidade**
- `popularity_score`: CombinaÃ§Ã£o de rating e stock (0.154-0.973)
- FÃ³rmula: `(rating/5) Ã— 0.7 + (stock_norm) Ã— 0.3`

#### 6. **One-Hot Encoding de Categorias** (49 colunas)
- `category_*`: Uma coluna binÃ¡ria para cada categoria Ãºnica
- Exemplo: `category_fantasy` (48 livros), `category_nonfiction` (110 livros)

## ğŸš€ Como Executar

### OpÃ§Ãµes de ExecuÃ§Ã£o:

```bash
# Pipeline completa (limpeza + features)
python scripts/run_pipeline.py

# Apenas limpeza
python scripts/run_pipeline.py --cleaning-only

# Apenas features (requer dados processados)
python scripts/run_pipeline.py --features-only

# Com configuraÃ§Ã£o customizada
python scripts/run_pipeline.py --config config.json

# Mostrar informaÃ§Ãµes
python scripts/run_pipeline.py --info

# Criar arquivo de configuraÃ§Ã£o padrÃ£o
python scripts/run_pipeline.py --create-config config.json
```

### ConfiguraÃ§Ã£o PadrÃ£o:

```json
{
  "input_file": "data/raw/all_books_with_images.csv",
  "processed_output": "data/processed/books_processed.csv", 
  "features_output": "data/features/books_features.csv",
  "default_category": "Outros",
  "problematic_categories": ["Add a comment", "Default"]
}
```

## ğŸ“Š Resultados da Ãšltima ExecuÃ§Ã£o

- **Registros processados:** 1000
- **Tempo de execuÃ§Ã£o:** 0.42s  
- **Features criadas:** 59
- **Categorias limpas:** 219
- **DistribuiÃ§Ã£o de preÃ§os:**
  - MÃ©dio (20-40): 401 livros (40.1%)
  - Alto (40-50): 205 livros (20.5%)
  - Premium (>50): 198 livros (19.8%)
  - Baixo (â‰¤20): 196 livros (19.6%)

## ğŸ”§ DependÃªncias

- `polars` - Processamento eficiente de dados
- `pydantic` - ValidaÃ§Ã£o e tipagem de dados
- `pathlib` - ManipulaÃ§Ã£o de caminhos
- `uuid` - GeraÃ§Ã£o de IDs Ãºnicos

## ğŸ“ˆ PrÃ³ximos Passos

Os dados estÃ£o prontos para:
- âœ… AnÃ¡lise exploratÃ³ria avanÃ§ada
- âœ… Modelagem preditiva de preÃ§os
- âœ… Sistema de recomendaÃ§Ã£o de livros
- âœ… API de consulta de dados

## ğŸ¯ Qualidade dos Dados

- **0 valores nulos** - Dataset Ã­ntegro
- **1000 IDs Ãºnicos** - IdentificaÃ§Ã£o consistente
- **59 features** - Rico conjunto para ML
- **ValidaÃ§Ã£o completa** - Dados confiÃ¡veis para anÃ¡lise

---

**Desenvolvido para Tech Challenge**  
*Pipeline otimizada com Polars + Pydantic*