"""
Pipeline de limpeza de dados para livros.

Este m√≥dulo cont√©m fun√ß√µes para realizar a limpeza b√°sica dos dados,
incluindo verifica√ß√£o de nulos, cria√ß√£o de ID √∫nico, tratamento de 
categorias problem√°ticas e transforma√ß√£o da coluna availability.
"""

import polars as pl
import random
from pathlib import Path
import logging
from typing import Tuple
from data_types import PipelineConfig, validate_polars_dataframe, get_raw_data_schema

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def check_null_values(df: pl.DataFrame) -> Tuple[pl.DataFrame, int]:
    """
    Verifica e reporta valores nulos no DataFrame.
    
    Args:
        df: DataFrame Polars com os dados
        
    Returns:
        Tuple[pl.DataFrame, int]: DataFrame limpo e n√∫mero de nulos encontrados
        
    Raises:
        ValueError: Se forem encontrados valores nulos nos dados
    """
    logger.info("Verificando valores nulos...")
    
    # Contar valores nulos por coluna
    null_counts = df.null_count()
    total_nulls = null_counts.sum_horizontal().sum()
    
    if total_nulls > 0:
        logger.error(f"Encontrados {total_nulls} valores nulos!")
        for col in df.columns:
            null_count = null_counts[col][0]
            if null_count > 0:
                logger.error(f"  {col}: {null_count} valores nulos")
        raise ValueError(f"Dataset cont√©m {total_nulls} valores nulos. Limpeza necess√°ria!")
    
    logger.info("‚úÖ Nenhum valor nulo encontrado!")
    return df, 0


def create_unique_id(df: pl.DataFrame) -> pl.DataFrame:
    """
    Cria uma coluna ID √∫nica como primeira coluna do dataset.
    
    Args:
        df: DataFrame Polars
        
    Returns:
        pl.DataFrame: DataFrame com coluna ID adicionada
    """
    logger.info("Criando IDs √∫nicos...")
    
    # Criar IDs √∫nicos usando n√∫meros aleat√≥rios
    n_rows = df.height
    ids = []
    used_ids = set()
    
    # Gerar IDs √∫nicos aleat√≥rios
    while len(ids) < n_rows:
        new_id = random.randint(100000, 999999)  # 6 d√≠gitos
        if new_id not in used_ids:
            ids.append(new_id)
            used_ids.add(new_id)
    
    # Adicionar coluna ID como primeira coluna
    df_with_id = df.with_columns(
        pl.Series("id", ids)
    ).select(["id"] + [col for col in df.columns])
    
    # Verificar se todos os IDs s√£o √∫nicos
    unique_ids = df_with_id["id"].n_unique()
    if unique_ids != n_rows:
        logger.warning(f"IDs duplicados detectados! {unique_ids} √∫nicos de {n_rows} total")
    
    logger.info(f"‚úÖ {n_rows} IDs √∫nicos aleat√≥rios criados!")
    return df_with_id


def clean_categories(df: pl.DataFrame, config: PipelineConfig) -> Tuple[pl.DataFrame, int]:
    """
    Trata categorias problem√°ticas substituindo por uma categoria padr√£o.
    
    Args:
        df: DataFrame Polars
        config: Configura√ß√£o da pipeline
        
    Returns:
        Tuple[pl.DataFrame, int]: DataFrame com categorias limpas e n√∫mero de registros alterados
    """
    logger.info("Limpando categorias problem√°ticas...")
    
    # Contar registros com categorias problem√°ticas
    problematic_mask = pl.col('category').is_in(config.problematic_categories)
    problematic_count = df.filter(problematic_mask).height
    
    if problematic_count > 0:
        logger.info(f"Encontradas {problematic_count} categorias problem√°ticas:")
        for cat in config.problematic_categories:
            count = df.filter(pl.col('category') == cat).height
            if count > 0:
                logger.info(f"  '{cat}': {count} registros")
        
        # Substituir categorias problem√°ticas
        df_clean = df.with_columns(
            pl.when(pl.col('category').is_in(config.problematic_categories))
            .then(pl.lit(config.default_category))
            .otherwise(pl.col('category'))
            .alias('category')
        )
        
        logger.info(f"‚úÖ {problematic_count} categorias substitu√≠das por '{config.default_category}'")
    else:
        df_clean = df
        logger.info("‚úÖ Nenhuma categoria problem√°tica encontrada!")
    
    return df_clean, problematic_count


def transform_availability(df: pl.DataFrame) -> pl.DataFrame:
    """
    Transforma a coluna availability de string para bin√°rio (yes=1, else=0).
    
    Args:
        df: DataFrame Polars
        
    Returns:
        pl.DataFrame: DataFrame com availability transformada
    """
    logger.info("Transformando coluna availability...")
    
    # Verificar valores √∫nicos antes da transforma√ß√£o
    unique_values = df['availability'].unique().to_list()
    logger.info(f"Valores √∫nicos encontrados: {unique_values}")
    
    # Transformar para bin√°rio
    df_transformed = df.with_columns(
        pl.when(pl.col('availability').str.to_lowercase() == 'yes')
        .then(1)
        .otherwise(0)
        .alias('availability')
    )
    
    # Verificar resultado
    transformed_values = df_transformed['availability'].unique().to_list()
    count_yes = df_transformed.filter(pl.col('availability') == 1).height
    count_no = df_transformed.filter(pl.col('availability') == 0).height
    
    logger.info(f"‚úÖ Availability transformada: {count_yes} 'yes' ‚Üí 1, {count_no} outros ‚Üí 0")
    
    return df_transformed


def validate_processed_data(df: pl.DataFrame) -> bool:
    """
    Valida os dados processados verificando tipos e constraints.
    
    Args:
        df: DataFrame processado
        
    Returns:
        bool: True se v√°lido, False caso contr√°rio
    """
    logger.info("Validando dados processados...")
    
    # Verificar se todas as colunas esperadas est√£o presentes
    expected_columns = [
        "id", "title", "price", "rating", "category", "image",
        "product_page", "availability", "stock", "image_base64"
    ]
    
    for col in expected_columns:
        if col not in df.columns:
            logger.error(f"Coluna obrigat√≥ria '{col}' n√£o encontrada!")
            return False
    
    # Verificar tipos e constraints
    try:
        # ID deve ser √∫nico
        if df["id"].n_unique() != df.height:
            logger.error("IDs n√£o s√£o √∫nicos!")
            return False
            
        # Pre√ßos devem ser positivos
        negative_prices = df.filter(pl.col('price') <= 0).height
        if negative_prices > 0:
            logger.error(f"{negative_prices} pre√ßos negativos ou zero encontrados!")
            return False
            
        # Ratings devem estar entre 1-5
        invalid_ratings = df.filter((pl.col('rating') < 1) | (pl.col('rating') > 5)).height
        if invalid_ratings > 0:
            logger.error(f"{invalid_ratings} ratings fora do range 1-5!")
            return False
            
        # Stock deve ser n√£o-negativo
        negative_stock = df.filter(pl.col('stock') < 0).height
        if negative_stock > 0:
            logger.error(f"{negative_stock} stocks negativos encontrados!")
            return False
            
        # Availability deve ser 0 ou 1
        invalid_availability = df.filter(~pl.col('availability').is_in([0, 1])).height
        if invalid_availability > 0:
            logger.error(f"{invalid_availability} valores inv√°lidos em availability!")
            return False
            
        logger.info("‚úÖ Dados processados v√°lidos!")
        return True
        
    except Exception as e:
        logger.error(f"Erro na valida√ß√£o: {str(e)}")
        return False


def run_cleaning_pipeline(input_path: str, output_path: str, config: PipelineConfig) -> Tuple[pl.DataFrame, dict]:
    """
    Executa a pipeline completa de limpeza de dados.
    
    Args:
        input_path: Caminho do arquivo de entrada
        output_path: Caminho do arquivo de sa√≠da
        config: Configura√ß√£o da pipeline
        
    Returns:
        Tuple[pl.DataFrame, dict]: DataFrame processado e estat√≠sticas
    """
    logger.info("Iniciando pipeline de limpeza...")
    logger.info(f"Entrada: {input_path}")
    logger.info(f"Sa√≠da: {output_path}")
    
    stats = {
        'total_records': 0,
        'null_records_found': 0,
        'categories_cleaned': 0,
        'processed_records': 0
    }
    
    try:
        # 1. Carregar dados brutos
        logger.info("Carregando dados brutos...")
        df = pl.read_csv(input_path)
        stats['total_records'] = df.height
        logger.info(f"Carregados {df.height} registros")
        
        # Validar schema dos dados brutos
        if not validate_polars_dataframe(df, get_raw_data_schema()):
            raise ValueError("Schema dos dados brutos inv√°lido!")
        
        # 2. Verificar valores nulos
        df, null_count = check_null_values(df)
        stats['null_records_found'] = null_count
        
        # 3. Criar ID √∫nico
        df = create_unique_id(df)
        
        # 4. Limpar categorias
        df, categories_cleaned = clean_categories(df, config)
        stats['categories_cleaned'] = categories_cleaned
        
        # 5. Transformar availability
        df = transform_availability(df)
        
        # 6. Validar dados processados
        if not validate_processed_data(df):
            raise ValueError("Valida√ß√£o dos dados processados falhou!")
        
        # 7. Salvar dados processados
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        df.write_csv(output_path)
        stats['processed_records'] = df.height
        
        logger.info("‚úÖ Pipeline de limpeza conclu√≠da!")
        logger.info(f"‚úÖ Dados salvos em: {output_path}")
        logger.info(f"üìä Estat√≠sticas: {stats}")
        
        return df, stats
        
    except Exception as e:
        logger.error(f"‚ùå Erro na pipeline de limpeza: {str(e)}")
        raise