"""
Pipeline principal para processamento de dados de livros.

Este m√≥dulo orquestra a execu√ß√£o completa da pipeline de ETL,
incluindo limpeza de dados e feature engineering.
"""

import polars as pl
import time
import logging
from pathlib import Path
from typing import Dict, Any
from data_types import PipelineConfig, PipelineStats
from cleaning_pipeline import run_cleaning_pipeline
from feature_pipeline import run_feature_pipeline

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def setup_directories(config: PipelineConfig) -> None:
    """
    Cria os diret√≥rios necess√°rios para a pipeline.
    
    Args:
        config: Configura√ß√£o da pipeline
    """
    logger.info("Configurando diret√≥rios...")
    
    directories = [
        Path(config.processed_output).parent,
        Path(config.features_output).parent
    ]
    
    for directory in directories:
        directory.mkdir(parents=True, exist_ok=True)
        logger.info(f"Diret√≥rio criado/verificado: {directory}")


def validate_input_file(input_path: str) -> bool:
    """
    Valida se o arquivo de entrada existe e √© v√°lido.
    
    Args:
        input_path: Caminho do arquivo de entrada
        
    Returns:
        bool: True se v√°lido, False caso contr√°rio
    """
    logger.info(f"Validando arquivo de entrada: {input_path}")
    
    input_file = Path(input_path)
    
    if not input_file.exists():
        logger.error(f"Arquivo de entrada n√£o encontrado: {input_path}")
        return False
    
    if not input_file.suffix.lower() == '.csv':
        logger.error(f"Arquivo deve ser CSV: {input_path}")
        return False
    
    try:
        # Tentar carregar e verificar se n√£o est√° vazio
        df = pl.read_csv(input_path)
        if df.height == 0:
            logger.error("Arquivo de entrada est√° vazio!")
            return False
        
        logger.info(f"‚úÖ Arquivo v√°lido: {df.height} registros, {len(df.columns)} colunas")
        return True
        
    except Exception as e:
        logger.error(f"Erro ao ler arquivo de entrada: {str(e)}")
        return False


def create_pipeline_stats(
    cleaning_stats: Dict,
    features_stats: Dict,
    execution_time: float
) -> PipelineStats:
    """
    Cria estat√≠sticas consolidadas da pipeline.
    
    Args:
        cleaning_stats: Estat√≠sticas da limpeza
        features_stats: Estat√≠sticas das features
        execution_time: Tempo total de execu√ß√£o
        
    Returns:
        PipelineStats: Estat√≠sticas consolidadas
    """
    return PipelineStats(
        total_records=cleaning_stats['total_records'],
        null_records_found=cleaning_stats['null_records_found'],
        duplicate_titles=0,  # Ser√° implementado se necess√°rio
        categories_cleaned=cleaning_stats['categories_cleaned'],
        processed_records=cleaning_stats['processed_records'],
        features_created=features_stats['features_created'],
        execution_time_seconds=execution_time
    )


def log_pipeline_summary(stats: PipelineStats, config: PipelineConfig) -> None:
    """
    Registra um resumo executivo da pipeline.
    
    Args:
        stats: Estat√≠sticas da pipeline
        config: Configura√ß√£o da pipeline
    """
    logger.info("=" * 60)
    logger.info("RESUMO EXECUTIVO DA PIPELINE")
    logger.info("=" * 60)
    logger.info(f"üìä Total de registros processados: {stats.total_records}")
    logger.info(f"üßπ Registros com nulos encontrados: {stats.null_records_found}")
    logger.info(f"üè∑Ô∏è  Categorias limpas: {stats.categories_cleaned}")
    logger.info(f"‚öôÔ∏è  Features criadas: {stats.features_created}")
    logger.info(f"‚è±Ô∏è  Tempo de execu√ß√£o: {stats.execution_time_seconds:.2f}s")
    logger.info(f"üìÅ Dados processados salvos em: {config.processed_output}")
    logger.info(f"üéØ Dados com features salvos em: {config.features_output}")
    logger.info("=" * 60)


def run_full_pipeline(config: PipelineConfig = None) -> PipelineStats:
    """
    Executa a pipeline completa de processamento de dados.
    
    Args:
        config: Configura√ß√£o da pipeline (opcional, usa padr√£o se None)
        
    Returns:
        PipelineStats: Estat√≠sticas da execu√ß√£o
        
    Raises:
        ValueError: Se houver erro na valida√ß√£o dos dados
        Exception: Para outros erros durante a execu√ß√£o
    """
    start_time = time.time()
    
    # Usar configura√ß√£o padr√£o se n√£o fornecida
    if config is None:
        config = PipelineConfig()
    
    logger.info("üöÄ INICIANDO PIPELINE DE PROCESSAMENTO DE DADOS")
    logger.info(f"Configura√ß√£o: {config.model_dump()}")
    
    try:
        # 1. Validar arquivo de entrada
        if not validate_input_file(config.input_file):
            raise ValueError(f"Arquivo de entrada inv√°lido: {config.input_file}")
        
        # 2. Configurar diret√≥rios
        setup_directories(config)
        
        # 3. Executar pipeline de limpeza
        logger.info("üßπ FASE 1: Pipeline de Limpeza")
        processed_df, cleaning_stats = run_cleaning_pipeline(
            config.input_file,
            config.processed_output,
            config
        )
        
        # 4. Executar pipeline de features
        logger.info("‚öôÔ∏è FASE 2: Pipeline de Feature Engineering")
        features_df, features_stats = run_feature_pipeline(
            config.processed_output,
            config.features_output,
            config
        )
        
        # 5. Calcular estat√≠sticas finais
        execution_time = time.time() - start_time
        final_stats = create_pipeline_stats(cleaning_stats, features_stats, execution_time)
        
        # 6. Log do resumo
        log_pipeline_summary(final_stats, config)
        
        logger.info("‚úÖ PIPELINE CONCLU√çDA COM SUCESSO!")
        
        return final_stats
        
    except Exception as e:
        execution_time = time.time() - start_time
        logger.error(f"‚ùå ERRO NA PIPELINE ap√≥s {execution_time:.2f}s: {str(e)}")
        raise


def run_cleaning_only(config: PipelineConfig = None) -> Dict:
    """
    Executa apenas a pipeline de limpeza.
    
    Args:
        config: Configura√ß√£o da pipeline
        
    Returns:
        Dict: Estat√≠sticas da limpeza
    """
    if config is None:
        config = PipelineConfig()
    
    logger.info("üßπ Executando apenas pipeline de limpeza...")
    
    if not validate_input_file(config.input_file):
        raise ValueError(f"Arquivo de entrada inv√°lido: {config.input_file}")
    
    setup_directories(config)
    
    _, cleaning_stats = run_cleaning_pipeline(
        config.input_file,
        config.processed_output,
        config
    )
    
    logger.info("‚úÖ Pipeline de limpeza conclu√≠da!")
    return cleaning_stats


def run_features_only(config: PipelineConfig = None) -> Dict:
    """
    Executa apenas a pipeline de feature engineering.
    Requer que os dados j√° tenham sido processados.
    
    Args:
        config: Configura√ß√£o da pipeline
        
    Returns:
        Dict: Estat√≠sticas das features
    """
    if config is None:
        config = PipelineConfig()
    
    logger.info("‚öôÔ∏è Executando apenas pipeline de features...")
    
    if not Path(config.processed_output).exists():
        raise ValueError(f"Dados processados n√£o encontrados: {config.processed_output}")
    
    setup_directories(config)
    
    _, features_stats = run_feature_pipeline(
        config.processed_output,
        config.features_output,
        config
    )
    
    logger.info("‚úÖ Pipeline de features conclu√≠da!")
    return features_stats


if __name__ == "__main__":
    # Executar pipeline com configura√ß√£o padr√£o
    try:
        stats = run_full_pipeline()
        print(f"Pipeline executada com sucesso em {stats.execution_time_seconds:.2f}s")
    except Exception as e:
        print(f"Erro na execu√ß√£o da pipeline: {str(e)}")
        exit(1)